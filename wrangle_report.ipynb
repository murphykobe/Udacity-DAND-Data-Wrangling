{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document I'll go through the data wrangling process for my project\n",
    "This process is consist of three part:\n",
    "- Gathering\n",
    "- Assessing\n",
    "- Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's three different data source for this project.\n",
    "- First for the archive tweet data, we just using an browser dowloading the csv file\n",
    "- Second, for the image prediciton, we are using requests package downloading programmatically with python, save it into the workspace\n",
    "- At last, the tweet retweet# and favorite# data we are using tweeter API and tweepy package to query each tweet data and saved into a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main chanllenge here I met is to gathering twitter data since it's running in the backgroud it's not easy to check the status, after I added exception handle and wait rate limit parameter it finally gathered all the data into the list I needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the data is been stored locally, it's being imported into jupyter notebook as dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality\n",
    "**tweet archive table**\n",
    "- Missing value in expanded_urls <br>\n",
    "    -> Using existing tweet_id recosntruct urls\n",
    "- Duplicate & error values in expanded_urls, repeated urls in one data entry <br>\n",
    "    -> same above\n",
    "- There's error in dog names, example like 'a' or 'an' is not a name, also a lot of entry is None <br>\n",
    "    -> using python string function remove non-capitalized names\n",
    "- 0 value on rating_numerator also there's values that not 10 for rating_denominator <br>\n",
    "    -> checking original tweet find correct rating value, remove non-rating tweet\n",
    "- Duplicate on dog types, there's serval dog have multiple properties which is incorrect <br>\n",
    "    -> checking original tweet text to find correct dog type\n",
    "- We only want original ratings (no retweets) that have images <br>\n",
    "    -> remove all the column have retweet data\n",
    "    \n",
    "**predition table**\n",
    "- Duplicate tweet_id column header <br>\n",
    "    -> join with tweet archive table\n",
    "- 2075 total entry, there's 200+ missing data entry compare to tweet archive table <br>\n",
    "    -> we will using inner join to exclude all the missing data\n",
    "- Dog type name convention is not the same (p1,p2,p3), there's lowercase also underscore into it <br>\n",
    "    -> using python string function to replace underscore '_' with ' '\n",
    "\n",
    "**count table**\n",
    "- there's duplicate data in this dataset need to be removed <br>\n",
    "    -> using df.drop_duplicates() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness\n",
    "- count table and prediction table can be inlcuded in the tweet archive table <br>\n",
    "    -> join all three table together\n",
    "- the dog types (doggo\tfloofer\tpupper\tpuppo) can be moved into 1 single column since every dog only have one of these properties <br>\n",
    "    -> melt these column into one column\n",
    "- Timestamp datatype and also other column datatype need to be changed <br>\n",
    "    -> changing column datatype to correct one using df.astype()\n",
    "- source in tweet archive table can be shortened iPhone,Vine,Twitter Web,TwitterDeck, help shorten the width of the column<br>\n",
    "    -> shorten the column with better column values<br>\n",
    "\n",
    "- There's decimal in img_num column which should be integer<br>\n",
    "    -> changing column datatype to correct one using df.astype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning is the last step, I used the method mentioned above which is both programmatically and manual(visually), of course the manual method have a lot of work and less effcient but sometimes it's a different view to find & solve problems that code/functions can't see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this project I found that data wrangling is definitely a must skill set to doing any data analaysis. Without tidy/clean data we can't get the best effect or correct result of any analysis or visualization. However I do feel like this is just a tip of the iceberg and there's much more to learn and practice. Practice makes perfect, with more experience I'll have a better chance facing any challenge in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
